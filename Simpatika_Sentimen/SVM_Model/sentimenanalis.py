# -*- coding: utf-8 -*-
"""SentimenAnalis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_sCzZoi42PXkreSfE5GB0umPN91FLn3c
"""

pip install Sastrawi

import pandas as pd
import re
import matplotlib.pyplot as plt
import Sastrawi
from wordcloud import WordCloud
import numpy as np
from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory, StopWordRemover, ArrayDictionary
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
from textblob import TextBlob
import nltk
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize

from google.colab import files
data = files.upload()

data = pd.read_csv("preprocessed_google_play_reviews.csv")
data

data.isnull().sum()

"""## Labeling"""

data_unclean = data.copy()

label = []
for index, row in data_unclean.iterrows():
    if row["score"] <= 2:
        label.append(0)  # Negatif
    elif row["score"] == 3:
        label.append(1)  # Netral
    else:
        label.append(2)  # Positif

data_unclean["label"] = label

data_unclean['label'].value_counts()

"""## Drop Duplicate"""

data_unclean = data_unclean.drop_duplicates()
data_unclean

"""## Drop Null"""

data_unclean['content'].isnull().sum()
data_unclean = data_unclean.dropna(subset=['content'])
data_unclean

def contains_number(text):
    return bool(re.search(r'\d', str(text)))

data_unclean['clean_text'] = data_unclean['content'].apply(contains_number)

data_unclean.isnull().sum()

"""## Normalize"""

# Normalisasi
norm = {
    " yg " : " yang ",
    " bgt " : " banget ",
    " bgt" : " banget",
    " bangat " : " banget ",
    " pisan " : " banget ",
    " trimakasih " : " terima kasih ",
    "terimakasih" : " terima kasih",
    " terimakasih" : " terima kasih",
    " terimakasih " : " terima kasih ",
    " kereen " : " keren ",
    "alhamdulillaah " : " Alhamdulillah ",
    " setinggitingginya " : " tinggi ",
    " app " : " aplikasi ",
    " apk " : " aplikasi ",
    " apl " : " aplikasi ",
    " nusantara" : " Indonesia",
    " he " : " ",
    " full " : " penuh ",
    "mudahmudahan " : " semoga ",
    " dgn " : " dengan ",
    "dg " : "dengan ",
    " tertakit " : " terkait ",
    " pengapdian " : " pengabdian ",
    "mantab " : "mantap ",
    " mantab " : " mantap ",
    " mantab" : " mantap",
    " mantaaf" : " mantap",
    " mantaaaaap" : " mantap ",
    "mantul" : "mantap",
    "mantaaab" : "mantap",
    "mantap jiwa" : "mantap",
    " ppg" : " ppg ",
    " tdk " : " tidak ",
    " sy " : " saya ",
    "tolong " : " tolong ",
    " berbeling belit " : " berbelit ",
    " guruguru " : " guru ",
    " ijasah " : " ijazah ",
    " terhusus " : " terkhusus ",
    " dn " : " dan ",
    "sngat " : "sangat ",
    " fahammmm" : " paham",
    " oke" : " baik",
    " blm " : " belum ",
    " lg" : " lagi",
    " sip " : " bagus ",
    " asyik " : " seru ",
    " fun " : " seru ",
    " best " : " terbaik ",
    "good" : "bagus",
    "bagussss" : "bagus",
    "bagussss " : "bagus ",
    " bagussss" : " bagus",
    " bagussss " : " bagus ",
    "jos" : "bagus",
    "like" : "suka",
    "informatig" : "informatif",
    "siiiiip" : "bagus",
    "good job" : "bagus",
    "nice" : "bagus",
    "jozz" : "bagus",
    "msh " : "masih ",
    "easy " : "mudah ",
    " parmudahkan " : " mudahkan ",
    " jg " : " juga ",
    " km " : " kami ",
    "smg " : "semoga ",
    " kl " : " kalau ",
    " lg " : " lagi ",
    " jd " : " jadi ",
    " dpt " : " dapat ",
    "sngt " : "sangat ",
    " sngt " : " sangat ",
    " se x" : " sekali",
    " mmbntu " : " membantu ",
    " memudhkn " : " memudahkan ",
    "praktis" : " praktis",
    " yg": " yang",
    " ok": " baik",
    " oke": " baik",
    " banget": " sangat",
    " aplikasinya": " aplikasi",
    " sip": " baik",
    " tdk": " tidak",
    " gak": " tidak",
    " bgt": " sangat",
    " trs": " terus",
    " dr": " dari",
    " krn": " karena",
    " sdh": " sudah",
    " blm": " belum",
    " tp": " tapi",
    " sy": " saya",
    " sbg": " sebagai",
    " utk": " untuk",
    " dlm": " dalam",
    " sj": " saja",
    " sm": " sama",
    " pd": " pada",
    " jd": " jadi",
    " mnrt": " menurut",
    " trmksh": " terima kasih",
    " mksh": " terima kasih",
    " kereen": " keren",
    "huebat": " hebat",
    "siippp": "sip",
    " trimakasih": " terima kasih",
    " aplkasi ": " aplikasi ", " apalikasi ": " aplikasi ", " apliasi ": " aplikasi ",
    " baguss ": " bagus ", " baguuss ": " bagus ", " bgs ": " bagus ",
    " mantabb ": " mantap ", " mantul ": " mantap ", " mantep ": " mantap ",
    " sgtt ": " sangat ", " sngt ": " sangat ", " sangaat ": " sangat ",

    " tdk ": " tidak ", " gpp ": " tidak apa-apa ", " trs ": " terus ",
    " blm ": " belum ", " udh ": " sudah ", " bgt ": " banget ", " bgt": " banget",
    " gak ": " tidak ", " ga ": " tidak ", " gabisa ": " tidak bisa ", " gaboleh ": " tidak boleh ",

    " log ": " login ", " server down ": " server tidak tersedia ",
    " nyesel ": " menyesal ",

    " best ": " terbaik ", " vibes ": " suasana ", " stylish ": " bergaya ",
    " worth ": " layak ", " simple ": " mudah ", " early ": " awal ",

    " kudu ": " harus ", " hrs ": " harus ", " jk ": " jika ", " dg ": " dengan ",
    " jd ": " jadi ", " problem ": " masalah ", " iru ": " itu ",
    " kereen": " keren",
    "huebat": " hebat",
    "siippp": "sip",
    " mendanlut": " mendownload",
    " logen": " login",
    " lht": " lihat",
    " yng": " yang",
    " laah": " lah",
    " tidakmpang": " gampang",
    "siip": "sip",
    " donlod": " download",
    " bs ": " bisa ",
    "gak ": " tidak ",
    "gkmana ": " bagaimana ",
    " betmanfaat ": " bermanfaat ",
    " dlm ": " dalam ",
    " aplikasix ": " aplikasinya ",
    " hax ": " hanya ",
    " gk ": " tidak ",
    " log in": " login ",
    " sllu ": " selalu ",
    " ggal ": " gagal ",
    " daribrowser ": " dari browser ",
    "error": "error ",
    "entah ": "tidak tau ",
    " lelet ": " lambat ",
    " dn ": " dan ",
    "kaga ": "tidak ",
    "eror ": "error ",
    " erorre ": " error ",
    " nggak ": " tidak ",
    " donlot ": " download ",
    " males ": " malas ",
    " tdk ": " tidak ",
    " gak ": " tidak ",
    " pasword": " password ",
    " dk ": " tidak ",
    " lgsung ": " langsung ",
    "knpa ": " kenapa ",
    " bisaaa ": " bisa ",
    "updet": "update",
    "login": "login ",
    "trims ": "terima ",
    " blm ": " belum ",
    " sklh ": " sekolah ",
    " tidk ": " tidak ",
    "singkron": "sinkron",
    "thankss": "terima kasih",
    "ngak": "tidak",
    "danlut": "download",
    "thanks": " terima kasih",
    " kpn ": " kapan ",
    "trimksh": "terima kasih",
    "ptkterimakasih": "ptk terima kasih",
    "mantaaap": "mantap",
    "baguuss": "bagus",
    "trima ksh": "terima kasih",
    "thanks": "terima kasih",
    "sipppppp": "sip",
    "mantaaaap": "mantap",
    "siiiippppp": "sip",
    "hebet": "hebat",
    "sipzzzz": "sip",
    "mantappp": "mantap",
    "mantaaap": "mantap",
    "kereeeeen": "keren",
    "mantaaabbbb": "mantab",
    "siiiip": "sip",
    "siiipppp": "sip",
    "bagusssss": "bagus",

}

def normalisasi(str_text):
  for i in norm:
    str_text = str_text.replace(i, norm[i])
  return str_text

data_unclean['content'] = data_unclean['content'].apply(lambda x: normalisasi(x))
data_unclean

data_unclean.to_csv('dataWithLabel.csv', index=False)

data_unclean['label'].sum()

"""## Stopwords"""

data_clean = data_unclean.copy()

more_stop_words = []

stop_words = StopWordRemoverFactory().get_stop_words()
new_array = ArrayDictionary(stop_words)
stop_words_remover_new = StopWordRemover(new_array)

def stopword(str_text):
  str_text = stop_words_remover_new.remove(str_text)
  return str_text


data_clean['content'] = data_clean['content'].apply(lambda x: stopword(x))
data_clean.head()

"""## Tokenize"""

tokenized = data_clean['content'].apply(lambda x:x.split())
tokenized

"""## Stemming"""

# def stemming(text_cleaning):
#   factory = StemmerFactory()
#   stemmer = factory.create_stemmer()
#   do = []
#   for w in text_cleaning:
#     dt = stemmer.stem(w)
#     do.append(dt)
#   d_clean = []
#   d_clean = " ".join(do)
#   print(d_clean)
#   return d_clean

# tokenized = tokenized.apply(stemming)


def stemming(text_cleaning):
    factory = StemmerFactory()
    stemmer = factory.create_stemmer()
    do = []
    for w in text_cleaning:
        if " " in w:  # Cek apakah ada spasi dalam string (lebih dari satu kata)
            dt = stemmer.stem(w)
        else:
            dt = w  # Jika hanya satu kata, tidak dilakukan stemming
        do.append(dt)
    d_clean = " ".join(do)
    print(d_clean)
    return d_clean
tokenized = tokenized.apply(stemming)

tokenized.to_csv('preprocessed_data.csv', index=False)
preprocessed_data = pd.read_csv('preprocessed_data.csv')
preprocessed_data.head()

"""### Tidak perlu dulu"""

data_stemming = data_stemming.rename(columns={"content": "content_stemming"})

at1 = data_unclean["content"]
at2 = data_stemming["content_stemming"]
att2 = data_unclean["label"]

result = pd.concat([at1, at2, att2], axis=1)

result

files.upload()

data_pre = pd.read_csv("normalize_stemmed_data.csv")
data_pre

clean_data = data_pre.dropna(subset=['stemmed_data'])
clean_data

tokenized = clean_data['stemmed_data'].apply(lambda x:x.split())
tokenized

tokenized.to_csv('databersih2.csv', index=False)
data_clean = pd.read_csv('databersih2.csv', encoding='latin1')
data_clean.head()

"""## TF-ID (Melihat total kata)"""

from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer

preprocessed_data.shape

data_unclean['label'].shape

at1 = preprocessed_data["content"].dropna().reset_index(drop=True)
at2 = data_unclean["label"].dropna().reset_index(drop=True)

result = pd.concat([at1, at2], axis=1)
print(result.shape)  # Harus (1644, 2)

print(type(at1))  # Pastikan ini pd.Series
print(type(at2))  # Pastikan ini pd.Series

result.isnull().sum()

result.shape

result = result.dropna(subset=['content'])

result

review = result['content']
cv = CountVectorizer()
term_fit = cv.fit(review)

term_fit.vocabulary_

term_frequency_all = term_fit.transform(review)
print(term_frequency_all)

ulasan_tf = review[1]
print(ulasan_tf)

term_frequency = term_fit.transform([ulasan_tf])
term_frequency

print(term_frequency)

dokumen = term_fit.transform(review)
tfidf_transformer = TfidfTransformer().fit(dokumen)
print(tfidf_transformer.idf_)

tfidf = tfidf_transformer.transform(term_frequency)
print(tfidf)

train_negatif = result[result["label"] == 0]
train_negatif

from wordcloud import WordCloud

all_text_s0 = ' '.join(word for word in train_negatif["content"])
wordcloud = WordCloud(colormap='Reds', width=1000, height=1000, mode='RGBA', background_color='white').generate(all_text_s0)
plt.figure(figsize=(20, 10))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title("Ulasan Negatif")
plt.margins(x=0, y=0)
plt.show()

train_netral = result[result["label"] == 1]
train_netral

all_text_s0 = ' '.join(word for word in train_netral["content"])
wordcloud = WordCloud(colormap='Blues', width=1000, height=1000, mode='RGBA', background_color='white').generate(all_text_s0)
plt.figure(figsize=(20, 10))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title("Ulasan Netral")
plt.margins(x=0, y=0)
plt.show()

train_positif = result[result["label"] == 2]
train_positif

all_text_s0 = ' '.join(word for word in train_positif["content"])
wordcloud = WordCloud(colormap='Greens', width=1000, height=1000, mode='RGBA', background_color='white').generate(all_text_s0)
plt.figure(figsize=(20, 10))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title("Ulasan Positif")
plt.margins(x=0, y=0)
plt.show()

sentimen_data = pd.value_counts(result["label"], sort=True)
sentimen_data.plot(kind='bar', color=['lightskyblue', 'red', 'green'])
plt.title("Bar Chart")
plt.show

"""## Oversampling"""

from imblearn.over_sampling import RandomOverSampler

oversampler = RandomOverSampler(random_state=42)
X_resampled, y_resampled = oversampler.fit_resample(result[['content']], result['label'])

X_resampled = X_resampled['content']

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(10,5))
sns.countplot(x=y_resampled, palette='coolwarm')
plt.title("Distribusi Kelas Setelah Oversampling")
plt.show()

"""## Training"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled,
                                                    test_size=0.3, stratify=y_resampled, random_state=30)

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(result['content'], result['label'],
                                                    test_size=0.3, stratify=result['label'], random_state=30)

import numpy as np

from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer(decode_error='replace', encoding='utf-8')

X_train = vectorizer.fit_transform(X_train)
X_test = vectorizer.transform(X_test)

print(X_train.shape)
print(X_test.shape)

X_train = X_train.toarray()

X_test = X_test.toarray()

print(X_train)

from sklearn.naive_bayes import GaussianNB

nb = GaussianNB()

from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.model_selection import GridSearchCV

cv_method = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=999)

params_NB = {'var_smoothing': np.logspace(0, -9, num=100)}
gscv_nb = GridSearchCV(estimator=nb,
                        param_grid=params_NB,
                        cv = cv_method,
                        verbose = 1,
                        scoring = 'accuracy')

gscv_nb.fit(X_train, y_train)
gscv_nb.best_params_

nb = GaussianNB(var_smoothing= 0.08111308307896872)

nb.fit(X_train, y_train)

y_pred_nb = nb.predict(X_test)

y_pred_nb

from sklearn.metrics import accuracy_score

accuracy = accuracy_score(y_test, y_pred_nb)
print(f"Accuracy: {accuracy:.4f}")

from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report

print('----- confusion matrix ------')
print(confusion_matrix(y_test, y_pred_nb))

print('----- classification report -----')
print(classification_report(y_test, y_pred_nb))